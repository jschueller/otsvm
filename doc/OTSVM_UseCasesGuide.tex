% 
% Permission is granted to copy, distribute and/or modify this document
% under the terms of the GNU Free Documentation License, Version 1.2
% or any later version published by the Free Software Foundation;
% with no Invariant Sections, no Front-Cover Texts, and no Back-Cover
% Texts.  A copy of the license is included in the section entitled "GNU
% Free Documentation License".




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Use Cases Guide}

This section presents the main functionalities of the module $otsvm$ in their context.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Which python modules to import ?}

In order to use the functionalities described in this documentation, it is necessary to import  : 
\begin{itemize}
   \item the openturns python module which gives access to the OpenTURNS functionalities.
   \item the $otsvm$ module which links the $openturns$ functionalities.
\end{itemize}

Python  script for this use case :
\begin{lstlisting}
# Load OpenTURNS 
from openturns import *
#Load the svm module
from otsvm import *
\end{lstlisting}

\subsection{UC : creation of a SVM regression algorithm}

The objective of this Use Case is to create a SVM Regression algorithm in order to create a metamodel.


Otsvm enables :

\begin{itemize}
 \item to set lists of tradeoff factors and kernel parameter with the methods setTradeoffFactor, setKernelParameter.
 \item to select the kernel type in this list : Linear Kernel, Polynomial Kernel, Sigmoid Kernel, RBF kernel.
 \item to compute the algorithm on an input and output samples.
 \item to compute the algorithm on an experiment plane and a function.
 \item to compute the algorithm on an input and output samples and an isoprobabilistic distribution.
\end{itemize}

We recommend for users to use the RBF Kernel ( the gaussian kernel ). Moreover, it is important to understand that the selection of parameters ( kernel parameter and tradeoff factor ) is primary. If you don't know 
what to take as parameters, you must take a wide range values, for example $tradeoff \in \{10^-5,10^-3,10^-1...10^3 \}$ $kernel\ parameter \in \{10^-15, 10^-13...,10^3 \}$. Normally, the algorithm always converges, but this can take a long while, mostly if you have a lot of parameters to test.\\

Python script for this UseCase :
\begin{lstlisting}
#create a function, here we create the Sobol function
dimension = 3
meanTh = 1.0
a = NumericalPoint(dimension)
inputVariables = Description(dimension)
outputVariables = Description(1)
outputVariables[0] = "y"
formula = Description(1)
formula[0] = "1.0"
covTh = 1.0
for i in range(dimension):
  a[i] = 0.5*i
  covTh = covTh * (1.0 + 1.0 / (3.0 * (1.0 + a[i])**2))
  inputVariables[i] = "xi" + str(i)
  formula[0] = formula[0] + " * ((abs(4.0 * xi" +str(i) + " -2.0) + " + 
  str(a[i]) + ") / (1.0 + " + str(a[i]) + "))"
covTh = covTh -1.0
model = NumericalMathFunction(inputVariables, outputVariables, formula)

#create the input distribution
RandomGenerator.SetSeed(0)
marginals = DistributionCollection(dimension)
for i in range(dimension):
  marginals[i] = Uniform(0.0, 1.0)
distribution = ComposedDistribution(marginals)

#create lists of kernel parameters and tradeoff factors
tradeoff = NumericalPoint([0.01,0.1,1,10,100,1000])
kernel = NumericalPoint([0.001,0.01,0.1,1,10,100])

#first example : create the problem with an input and output samples:
#first, we create samples
dataIn = distribution.getNumericalSample(250)
dataOut = model(dataIn)
#second, we create our svm regression object, we must select the third parameter 
#in an enumerate in the list { NormalRBF, Linear, Sigmoid, Polynomial }
Regression = SVMRegression(dataIn, dataOut, LibSVM.NormalRBF)
#third, we set kernel parameter and tradeoff factor
Regression.setTradeoffFactor(tradeoff)
Regression.setKernelParameter(kernel)
# Perform the algorithm
Regression.run()
# Stream out the results
SVMRegressionResult = Regression.getResult()
# get the residual error
residual = result.getResiduals()
# get the relative error
relativeError = result.getRelativeErrors()

#second example : create the problem with an experiment plane:
#first, we create the plane
myPlane = MonteCarloExperiment(distribution, 250)
myExperiment = Experiment(myPlane, "example")
#second, we create our svm regression object, the first parameter is the function
Regression2 = SVMRegression(model, myExperiment, 
LibSVM.Linear)
#third, we set kernel parameter and tradeoff factor
Regression2.setTradeoffFactor(tradeoff)
Regression2.setKernelParameter(kernel)
# Perform the algorithm
Regression2.run()
# Stream out the results
SVMRegressionResult = Regression2.getResult()
# get the residual error
residual = result.getResiduals()
# get the relative error
relativeError = result.getRelativeErrors()

#third example : create the problem with an isoprobabilistic distribution
#first, we create our distribution
marginals = DistributionCollection(dimension)
for i in range(dimension):
  marginals[i] = Uniform(0.0, 1.0)
distribution = ComposedDistribution(marginals)
#second, we create input and output samples
dataIn = distribution.getNumericalSample(250)
dataOut = model(dataIn)
#third, we create our svm regression
Regression3 = SVMRegression(dataIn,dataOut,distribution,
LibSVM.Polynomial) 
#and to finish, we set kernel parameter and tradeoff factor
Regression3.setTradeoffFactor(tradeoff)
Regression3.setKernelParameter(kernel)
# Perform the algorithm
Regression3.run()
# Stream out the results
SVMRegressionResult = Regression3.getResult()
# get the residual error
residual = result.getResiduals()
# get the relative error
relativeError = result.getRelativeErrors()

#fourth example is here to present you the SVMResourceMap class. 
#Users can fix others parameters like the degree and the constant of the 
#Polynomial Kernel,the cacheSize, the number of folds or the epsilon
#first, we create samples
dataIn = distribution.getNumericalSample(250)
dataOut = model(dataIn)
#second, we create our svm regression object
#here, we select the Polynomial Kernel but by default his degree is 3. We want a 
#degree of 2
ResourceMap.Set("LibSVM-DegreePolynomialKernel","2")
#now the degree of the Polynomial kernel is 2
Regression = SVMRegression(dataIn, dataOut, LibSVM.Polynomial)
#third, we set kernel parameter and tradeoff factor
Regression.setTradeoffFactor(tradeoff)
Regression.setKernelParameter(kernel)
# Perform the algorithm
Regression.run()
# Stream out the results
SVMRegressionResult = Regression.getResult()
# get the residual error
residual = result.getResiduals()
# get the relative error
relativeError = result.getRelativeErrors()




\end{lstlisting}


\subsection{UC : Creation of a Classification algorithm}

The objective of this Use Case is to create a SVM Classification algorithm in order to build a metamodel that separates datas in 2 classes.

Otsvm enables to :
\begin{itemize}
 \item to set lists of tradeoff factors and kernel parameter with the methods setTradeoffFactor, setKernelParameter.
 \item to select the kernel type in this list : Linear Kernel, Polynomial Kernel, Sigmoid Kernel, RBF kernel.
 \item to compute the algorithm on an input and output samples.
\end{itemize}

Python script for this UseCase :
\begin{lstlisting}
#this example uses a csv file with the datas for the classification
#we retreive the sample from the file sample.csv
path = os.path.abspath(os.path.dirname(__file__))
dataInOut = NumericalSample().ImportFromCSVFile(path + "/sample.csv")

#we create dataIn and dataOut
dataIn=NumericalSample(861,2)
dataOut=NumericalSample(861,1)

#we build the input Sample and the output Sample because we must separate dataInOut
for i in range(861):
  a=dataInOut[i]
  b=NumericalPoint(2)
  b[0]=a[1]
  b[1]=a[2]
  dataIn[i]=b
  dataOut[i]=int(a[0])

#list of C parameter 
cp=NumericalPoint([0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100])
#list of gamma parameter in kernel function
gamma=NumericalPoint([0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100])

#create the Classification Problem 
Regression=LibSVMClassification(dataIn,dataOut)
Regression.setKernelType(LibSVM.NormalRbf)
Regression.setTradeoffFactor(cp)
Regression.setKernelParameter(gamma) 

#compute the classification
Regression.run()
print "#######################"
print "Results with Samples I/O"
print "Accuracy(p.c.)=",Regression.getAccuracy()  
\end{lstlisting}








